{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv_s_4/Relu:0\", shape=(?, 1, 1, 64), dtype=float32)\n",
      "Tensor(\"conv_top/BatchNorm/batchnorm/add_1:0\", shape=(?, 1, 1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "total_layers = 25 #Specify how deep we want our network\n",
    "units_between_stride = total_layers / 5\n",
    "\n",
    "def resUnit(input_layer,i):\n",
    "    with tf.variable_scope(\"res_unit\"+str(i)):\n",
    "        part1 = slim.batch_norm(input_layer,activation_fn=None)\n",
    "        part2 = tf.nn.relu(part1)\n",
    "        part3 = slim.conv2d(part2,64,[3,3],activation_fn=None)\n",
    "        part4 = slim.batch_norm(part3,activation_fn=None)\n",
    "        part5 = tf.nn.relu(part4)\n",
    "        part6 = slim.conv2d(part5,64,[3,3],activation_fn=None)\n",
    "        output = input_layer + part6\n",
    "        return output\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32,name='input')\n",
    "label_layer = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "label_oh = slim.layers.one_hot_encoding(label_layer,10)\n",
    "\n",
    "layer1 = slim.conv2d(input_layer,64,[3,3],normalizer_fn=slim.batch_norm,scope='conv_'+str(0))\n",
    "for i in range(5):\n",
    "    for j in range(units_between_stride):\n",
    "        layer1 = resUnit(layer1,j + (i*units_between_stride))\n",
    "    layer1 = slim.conv2d(layer1,64,[3,3],stride=[2,2],normalizer_fn=slim.batch_norm,scope='conv_s_'+str(i))\n",
    "print layer1   \n",
    "top = slim.conv2d(layer1,10,[3,3],normalizer_fn=slim.batch_norm,activation_fn=None,scope='conv_top')\n",
    "print top\n",
    "output = slim.layers.softmax(slim.layers.flatten(top))\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(label_oh * tf.log(output) + 1e-10, reduction_indices=[1]))\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "update = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #Alexnet last layer\n",
    "    with tf.variable_scope('fc7'):\n",
    "                num_inputs = int(logits.get_shape()[1])\n",
    "                num_outputs = 1000\n",
    "                tr_vars['fc7w'] = W\n",
    "                tr_vars['fc7b'] = b\n",
    "              \n",
    "                self.fc7 = tf.add(tf.matmul(self.logit_dropout, tr_vars['fc7w'],transpose_a =True), tr_vars['fc7b'],\n",
    "                                  name='fc')\n",
    "                if use_batch_norm:\n",
    "                    print 'Using batch_norm after FC7'\n",
    "                    self.fc7_bn = tflayers.batch_norm(self.fc7, decay=0.999,\n",
    "                                                      is_training=self.is_phase_train,\n",
    "                                                      trainable=False)\n",
    "                    out = self.fc7_bn\n",
    "                else:\n",
    "                    out = self.fc7\n",
    "\n",
    "                self.fc7_relu = tf.nn.relu(out, name='relu')\n",
    "\n",
    "                self.fc7_keep_prob = tf.placeholder_with_default(1.0, tuple(),\n",
    "                                                                 name='keep_prob_pl')\n",
    "                self.fc7_dropout = tf.nn.dropout(self.fc7_relu, self.fc7_keep_prob, name='dropout')    \n",
    "        self.logits = self.fc7\n",
    "        with tf.variable_scope('output'):\n",
    "            self.prob = tf.nn.softmax(self.fc7, name='prob')\n",
    "        self.trainable_vars = tr_vars\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " with argscope(Conv2D, nl=tf.identity, use_bias=False,\n",
    "                      W_init=variance_scaling_initializer(mode='FAN_OUT')), \\\n",
    "                argscope([Conv2D, MaxPooling, GlobalAvgPooling, BatchNorm], data_format='NCHW'):\n",
    "            logits = (LinearWrap(self.x)\n",
    "                      .Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)\n",
    "                      .MaxPooling('pool0', shape=3, stride=2, padding='SAME')\n",
    "                      .apply(layer, 'group0', block_func, 64, defs[0], 1, first=True)\n",
    "                      .apply(layer, 'group1', block_func, 128, defs[1], 2)\n",
    "                      .apply(layer, 'group2', block_func, 256, defs[2], 2)\n",
    "                      .apply(layer, 'group3', block_func, 512, defs[3], 2)\n",
    "                      .BNReLU('bnlast')\n",
    "                      .GlobalAvgPooling('gap')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('fc7'):\n",
    "                num_inputs = int(logits.get_shape()[1])\n",
    "                print num_inputs\n",
    "                num_outputs = 512\n",
    "                tr_vars['fc7w'], tr_vars['fc7b'] = \\\n",
    "                    self.get_fc_weights(18, net_data, num_inputs, num_outputs)\n",
    "                layer_index += 1\n",
    "                self.fc7 = tf.add(tf.matmul(fc6_dropout, tr_vars['fc7w']), tr_vars['fc7b'],\n",
    "                                  name='fc')\n",
    "                if use_batch_norm:\n",
    "                    print 'Using batch_norm after FC7'\n",
    "                    self.fc7_bn = tflayers.batch_norm(self.fc7, decay=0.999,\n",
    "                                                      is_training=self.is_phase_train,\n",
    "                                                      trainable=False)\n",
    "                    out = self.fc7_bn\n",
    "                else:\n",
    "                    out = self.fc7\n",
    "\n",
    "                self.fc7_relu = tf.nn.relu(out, name='relu')\n",
    "\n",
    "                self.fc7_keep_prob = tf.placeholder_with_default(1.0, tuple(),\n",
    "                                                                 name='keep_prob_pl')\n",
    "                self.fc7_dropout = tf.nn.dropout(self.fc7_relu, self.fc7_keep_prob, name='dropout')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    " logits = (LinearWrap(self.x)\n",
    "                      .Conv2D('conv0', 64, 7, stride=2, nl=BNReLU)\n",
    "                      .MaxPooling('pool0', shape=3, stride=2, padding='SAME')\n",
    "                      .apply(layer, 'group0', block_func, 64, defs[0], 1, first=True)\n",
    "                      .apply(layer, 'group1', block_func, 128, defs[1], 2)\n",
    "                      .apply(layer, 'group2', block_func, 256, defs[2], 2)\n",
    "                      .apply(layer, 'group3', block_func, 512, defs[3], 2)\n",
    "                      .BNReLU('bnlast')\n",
    "                      .GlobalAvgPooling('gap')\n",
    "                      .FullyConnected('fc0', 512, nl=tf.nn.relu)\n",
    "                      .Dropout('dropout', 0.5)\n",
    "                      .FullyConnected('linear', 1000, nl=tf.identity)())\n",
    "            prob = tf.nn.softmax(logits, name='prob')\n",
    "            logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'randn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c99c8f455526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'randn'"
     ]
    }
   ],
   "source": [
    "inputs = tf.fill([1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['group3.block1.conv1.weight', 'group2.block0.conv0.bias', 'group3.block0.conv1.weight', 'group2.block1.conv1.weight', 'group1.block0.conv1.bias', 'group0.block0.conv0.weight', 'group2.block0.conv0.weight', 'group1.block0.conv_dim.weight', 'group3.block1.conv0.bias', 'group3.block1.conv1.bias', 'group0.block0.conv1.bias', 'group1.block0.conv0.bias', 'group0.block1.conv1.weight', 'group3.block0.conv1.bias', 'group2.block0.conv1.weight', 'group1.block0.conv0.weight', 'conv0.weight', 'group3.block0.conv_dim.weight', 'group0.block0.conv0.bias', 'fc.weight', 'group1.block1.conv1.bias', 'group0.block1.conv0.weight', 'group1.block1.conv0.bias', 'conv0.bias', 'group1.block0.conv1.weight', 'group2.block1.conv1.bias', 'group1.block1.conv0.weight', 'group0.block1.conv1.bias', 'group3.block1.conv0.weight', 'group2.block1.conv0.bias', 'group0.block0.conv1.weight', 'group0.block1.conv0.bias', 'group3.block0.conv0.weight', 'group2.block1.conv0.weight', 'fc.bias', 'group1.block1.conv1.weight', 'group3.block0.conv0.bias', 'group2.block0.conv1.bias', 'group2.block0.conv_dim.weight']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d6f59cc5d577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0my_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs_tf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# check that difference between PyTorch and Tensorflow is small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "def g(inputs, params):\n",
    "    '''Basicblock original ResNet model definition\n",
    "       the same for resnet-18 and resnet-34 in functional-zoo\n",
    "    '''\n",
    "    \n",
    "    def tr(v):\n",
    "        if v.ndim == 4:\n",
    "            return v.transpose(2,3,1,0)\n",
    "        elif v.ndim == 2:\n",
    "            return v.transpose()\n",
    "        return v\n",
    "    params = {k: tf.constant(tr(v)) for k, v in params.iteritems()}\n",
    "    \n",
    "    def conv2d(x, params, name, stride=1, padding=0):\n",
    "        x = tf.pad(x, [[0,0],[padding,padding],[padding,padding],[0,0]])\n",
    "        z = tf.nn.conv2d(x, params['%s.weight'%name], [1,stride,stride,1],\n",
    "                         padding='VALID')\n",
    "        if '%s.bias'%name in params:\n",
    "            return tf.nn.bias_add(z, params['%s.bias'%name])\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "    def group(input, params, base, stride, n):\n",
    "        o = input\n",
    "        for i in range(0,n):\n",
    "            b_base = ('%s.block%d.conv') % (base, i)\n",
    "            x = o\n",
    "            o = conv2d(x, params, b_base + '0', padding=1, stride=i==0 and stride or 1)\n",
    "            o = tf.nn.relu(o)\n",
    "            o = conv2d(o, params, b_base + '1', padding=1)\n",
    "            if i == 0 and stride != 1:\n",
    "                o += conv2d(x, params, b_base + '_dim', stride=stride)\n",
    "            else:\n",
    "                o += x\n",
    "            o = tf.nn.relu(o)\n",
    "        return o\n",
    "    \n",
    "    # determine network size by parameters\n",
    "    blocks = [sum([re.match('group%d.block\\d+.conv0.weight'%j, k) is not None\n",
    "                   for k in params.keys()]) for j in range(4)]\n",
    "\n",
    "    o = conv2d(inputs, params, 'conv0', 2, 3)\n",
    "    o = tf.nn.relu(o)\n",
    "    o = tf.pad(o, [[0,0], [1,1], [1,1], [0,0]])\n",
    "    o = tf.nn.max_pool(o, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID')\n",
    "    o_g0 = group(o, params, 'group0', 1, blocks[0])\n",
    "    o_g1 = group(o_g0, params, 'group1', 2, blocks[1])\n",
    "    o_g2 = group(o_g1, params, 'group2', 2, blocks[2])\n",
    "    o_g3 = group(o_g2, params, 'group3', 2, blocks[3])\n",
    "    o = tf.nn.avg_pool(o_g3, ksize=[1,7,7,1], strides=[1,1,1,1], padding='VALID')\n",
    "    o = tf.reshape(o, [-1,512])\n",
    "    o = tf.matmul(o, params['fc.weight']) + params['fc.bias']\n",
    "    return o\n",
    "\n",
    "params = hkl.load('resnet-18-export.hkl')\n",
    "print params.keys()\n",
    "inputs_tf = tf.placeholder(tf.float32, shape=[None,224,224,3])\n",
    "\n",
    "out = g(inputs_tf, params)\n",
    "\n",
    "sess = tf.Session()\n",
    "y_tf = sess.run(out, feed_dict={inputs_tf: inputs.permute(0,2,3,1).numpy()})\n",
    "\n",
    "# check that difference between PyTorch and Tensorflow is small\n",
    "assert np.abs(y_tf - y.data.numpy()).max() < 1e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
